Return-Path: <nilekim@gmail.com>
X-Spam-Checker-Version: SpamAssassin 3.1.3 (2006-06-01) on yquem.inria.fr
X-Spam-Level: **
X-Spam-Status: No, score=2.8 required=5.0 tests=DNS_FROM_RFC_POST,
	HTML_MESSAGE,SPF_NEUTRAL autolearn=disabled version=3.1.3
X-Original-To: caml-list@yquem.inria.fr
Delivered-To: caml-list@yquem.inria.fr
Received: from mail3-relais-sop.national.inria.fr (mail3-relais-sop.national.inria.fr [192.134.164.104])
	by yquem.inria.fr (Postfix) with ESMTP id AABDBBBC4
	for <caml-list@yquem.inria.fr>; Fri, 20 Feb 2009 23:43:49 +0100 (CET)
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: ArcBAMTAnknRVd0VkGdsb2JhbACCQDGKG4cNPwEBAQEJCQwHEQOuIIEFj0EBAwEDhAwGhiQ
X-IronPort-AV: E=Sophos;i="4.38,243,1233529200"; 
   d="scan'208";a="23304696"
Received: from mail-qy0-f21.google.com ([209.85.221.21])
  by mail3-smtp-sop.national.inria.fr with ESMTP; 20 Feb 2009 23:43:48 +0100
Received: by qyk14 with SMTP id 14so4624265qyk.9
        for <caml-list@inria.fr>; Fri, 20 Feb 2009 14:43:47 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=gamma;
        h=domainkey-signature:mime-version:received:in-reply-to:references
         :date:message-id:subject:from:to:cc:content-type;
        bh=q+cWrcFw5mn2CZRjjW4WNK2/2ciclAv5o+FUJ+ByEBA=;
        b=pCPYULmmazNyx68AumluwYGVgZyp0lrb7Zwtsx/+G4fN4ajWrYXuVbpZhrLLx9NJEj
         9QhuoPX7SpROWSssIkso4EvAJEb/6CUI68fMPaUW9AVkJVT3pkRX8sYOirr7DfD9YTAD
         4X/1aH8xjlJL6FX8FVELirHoGuJwLSbQtt2xg=
DomainKey-Signature: a=rsa-sha1; c=nofws;
        d=gmail.com; s=gamma;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        b=p/5FDaTczRSE9SBzrEHlypfRVOXaAtI0VQ9hxrDXbcw6fWi+FosQVUt5ysjkA1lUhr
         WWf9dNQGwZ6XsSBt+MbrQCBh43tjPKYXf780qk1ker7ow/+7Z1XwazqMLLB1XifuML/u
         72YBjVNRnh1gbrtrBjzP9FLKn47HqU+t8+W9Q=
MIME-Version: 1.0
Received: by 10.224.20.81 with SMTP id e17mr2399580qab.40.1235169826932; Fri, 
	20 Feb 2009 14:43:46 -0800 (PST)
In-Reply-To: <ef6818c20902201430y7473f4d1k3563995f3a154792@mail.gmail.com>
References: <243054520902200740j1d1874adib27645f6e090b073@mail.gmail.com>
	 <499EFA7E.7010400@inria.fr>
	 <243054520902201153q4efbe1f0j686bd7212b515c03@mail.gmail.com>
	 <2a1a1a0c0902201423o5ae7dd3dhb5c64da3c3255a69@mail.gmail.com>
	 <ef6818c20902201430y7473f4d1k3563995f3a154792@mail.gmail.com>
Date: Fri, 20 Feb 2009 17:43:46 -0500
Message-ID: <2a1a1a0c0902201443k6c971df0sfa5a469ab0276069@mail.gmail.com>
Subject: Re: [Caml-list] speeding up matrix multiplication (newbie question)
From: Mike Lin <nilekim@gmail.com>
To: "Will M. Farr" <farr@mit.edu>
Cc: Erick Matsen <matsen@berkeley.edu>, caml-list@inria.fr
Content-Type: multipart/alternative; boundary=0015175cde66c586f50463616295
X-Spam: no; 0.00; ocaml:01 allocator:01 malloc:01 ocaml:01 arrays:01 unboxed:01 arrays:01 multiplying:01 matrices:01 overwrite:01 matrices:01 bigarrays:01 blas:01 lacaml:01 bigarray:01 

--0015175cde66c586f50463616295
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: 7bit

These are good points. I tend to compulsively eliminate any kind of memory
allocation from my numerical loops -- it's true the OCaml allocator is a lot
faster than malloc, but you could end up repaying a lot of that back to the
GC later!
The silly library I sent out does operate on OCaml float arrays (which are
unboxed and directly accessible as double arrays to C, though I'm not sure
if it's totally kosher to use 'em that way). Something to kick around...
Mike

On Fri, Feb 20, 2009 at 5:30 PM, Will M. Farr <farr@mit.edu> wrote:

> Mike and Erick,
>
> In some of my work, I've got code which is constantly creating and
> multiplying 4x4 matrices (Lorentz transforms).  I usually write in a
> functional style, so I do not generally overwrite old matrices with
> the multiplication results.  I have discovered that, at these sizes,
> it's about a factor of two to three faster to do this with OCaml
> arrays for the matrices rather than the Bigarrays you would need to
> use to interface with the BLAS in Lacaml or ocamlgsl.  The reason is
> that the memory block for a bigarray is malloc-ed, which is extremely
> slow compared to the OCaml native allocation.  That overhead
> completely eliminates the advantage of going through C or Fortran
> libraries.
>
> The ideal solution would be to use OCaml native float arrays (fast
> allocation) with LAPACK (fast matrix multiply), but I'm satisfied with
> the performance of my code, and too lazy to put out the effort to
> implement the C glue for this.
>
> Just thought you might appreciate another data point.  I have no idea
> where the turnover is between 4x4 and 61x61 :).
>
> Will
>
> 2009/2/20 Mike Lin <nilekim@gmail.com>:
> > Erick, we should compare notes sometime. I have a lot of code for doing
> this
> > kind of stuff (I am working on empirical codon models with 61x61 rate
> > matrices). The right way to speed up matrix-vector operations is to use
> BLAS
> > via either Lacaml or ocamlgsl. But if, like me, you like to
> > counterproductively fiddle around with stupid things, here's a little
> ocaml
> > library I wrote that links to a C function to do vector-vector dot using
> > SSE2 vectorization.
> >
> > http://www.broad.mit.edu/~mlin/for_others/SuperFast_dot.zip<http://www.broad.mit.edu/%7Emlin/for_others/SuperFast_dot.zip>
> >
> > IIRC in my tests it gave about 50% speedup vs. the obvious C code and
> 100%
> > speedup vs. the obvious ocaml code. But, I am doing 61x61 and I'm not
> sure
> > if the speedup scales down to 20x20 or especially 4x4.
> >
> > Mike
> >
> > On Fri, Feb 20, 2009 at 2:53 PM, Erick Matsen <matsen@berkeley.edu>
> wrote:
> >>
> >> Wow, once again I am amazed by the vitality of this list. Thank you
> >> for your suggestions.
> >>
> >> Here is the context: we are interested in calculating the likelihood
> >> of taxonomic placement of short "metagenomics" sequence fragments from
> >> unknown organisms in the ocean. We start by assuming a model of
> >> sequence evolution, which is a reversible Markov chain. The taxonomy
> >> is represented as a tree, and the sequence information is a collection
> >> of likelihoods of sequence identities. As we move up the tree, these
> >> sequences "evolve" by getting multiplied by the exponentiated
> >> instantaneous Markov matrix.
> >>
> >> The matrices are of the size of the sequence model: 4x4 when looking
> >> at nucleotides, and 20x20 when looking at proteins.
> >>
> >> The bottleneck is (I mis-spoke before) that we are multiplying many
> >> length-4 or length-20 vectors by a collection of matrices which
> >> represent the time evolution of those sequences as follows.
> >>
> >> Outer loop:
> >>  modify the amount of time each markov process runs
> >>  exponentiate the rate matrices to get transition matrices
> >>
> >>  Recur over the tree, starting at the leaves:
> >>    at a node, multiply all of the daughter likelihood vectors together
> >>    return the multiplication of that product by the trasition matrix
> >> (bottleneck!)
> >>
> >> The trees are on the order of 50 leaves, and there are about 500
> >> likelihood vectors done at once.
> >>
> >> All of this gets run on a big cluster of Xeons. It's not worth
> >> parallelizing because we are running many instances of this process
> >> already, which fills up the cluster nodes.
> >>
> >> So far I have been doing the simplest thing possible, which is just to
> >> multiply the matrices out like \sum_j a_ij v_j. Um, this is a bit
> >> embarassing.
> >>
> >> let mul_vec m v =
> >>    if Array.length v <> n_cols m then
> >>      failwith "mul_vec: matrix size and vector size don't match!";
> >>    let result = Array.create (n_rows m) N.zero in
> >>    for i=0 to (n_rows m)-1 do
> >>      for j=0 to (n_cols m)-1 do
> >>        result.(i) <- N.add result.(i) (N.mul (get m i j) v.(j))
> >>      done;
> >>    done;
> >>    result
> >>
> >> I have implemented it in a functorial way for flexibility. N is the
> >> number class. How much improvement might I hope for if I make a
> >> dedicated float vector multiplication function? I'm sorry, I know
> >> nothing about "boxing." Where can I read about that?
> >>
> >>
> >> Thank you again,
> >>
> >> Erick
> >>
> >>
> >>
> >> On Fri, Feb 20, 2009 at 10:46 AM, Xavier Leroy <Xavier.Leroy@inria.fr>
> >> wrote:
> >> >> I'm working on speeding up some code, and I wanted to check with
> >> >> someone before implementation.
> >> >>
> >> >> As you can see below, the code primarily spends its time multiplying
> >> >> relatively small matrices. Precision is of course important but not
> >> >> an incredibly crucial issue, as the most important thing is relative
> >> >> comparison between things which *should* be pretty different.
> >> >
> >> > You need to post your matrix multiplication code so that the regulars
> >> > on this list can tear it to pieces :-)
> >> >
> >> > From the profile you gave, it looks like you parameterized your matrix
> >> > multiplication code over the + and * operations over matrix elements.
> >> > This is good for genericity but not so good for performance, as it
> >> > will result in more boxing (heap allocation) of floating-point values.
> >> > The first thing you should try is write a version of matrix
> >> > multiplication that is specialized for type "float".
> >> >
> >> > Then, there are several ways to write the textbook matrix
> >> > multiplication algorithm, some of which perform less boxing than
> >> > others.  Again, post your code and we'll let you know.
> >> >
> >> >> Currently I'm just using native (double-precision) ocaml floats and
> >> >> the native ocaml arrays for a first pass on the problem.  Now I'm
> >> >> thinking about moving to using float32 bigarrays, and I'm hoping
> >> >> that the code will double in speed. I'd like to know: is that
> >> >> realistic? Any other suggestions?
> >> >
> >> > It won't double in speed: arithmetic operations will take exactly the
> >> > same time in single or double precision.  What single-precision
> >> > bigarrays buy you is halving the memory footprint of your matrices.
> >> > That could result in better cache behavior and therefore slightly
> >> > better speed, but it depends very much on the sizes and number of your
> >> > matrices.
> >> >
> >> > - Xavier Leroy
> >> >
> >>
> >> _______________________________________________
> >> Caml-list mailing list. Subscription management:
> >> http://yquem.inria.fr/cgi-bin/mailman/listinfo/caml-list
> >> Archives: http://caml.inria.fr
> >> Beginner's list: http://groups.yahoo.com/group/ocaml_beginners
> >> Bug reports: http://caml.inria.fr/bin/caml-bugs
> >
> >
> > _______________________________________________
> > Caml-list mailing list. Subscription management:
> > http://yquem.inria.fr/cgi-bin/mailman/listinfo/caml-list
> > Archives: http://caml.inria.fr
> > Beginner's list: http://groups.yahoo.com/group/ocaml_beginners
> > Bug reports: http://caml.inria.fr/bin/caml-bugs
> >
> >
>

--0015175cde66c586f50463616295
Content-Type: text/html; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable

These are good points. I tend to compulsively eliminate any kind of memory =
allocation from my numerical loops -- it&#39;s true the OCaml allocator is =
a lot faster than malloc, but you could end up repaying a lot of that back =
to the GC later!<br>

The silly library I sent out does operate on OCaml float arrays (which are =
unboxed and directly accessible as double arrays to C, though I&#39;m not s=
ure if it&#39;s totally kosher to use &#39;em that way). Something to kick =
around...<br>
Mike<br><br><div class=3D"gmail_quote">On Fri, Feb 20, 2009 at 5:30 PM, Wil=
l M. Farr <span dir=3D"ltr">&lt;<a href=3D"mailto:farr@mit.edu">farr@mit.ed=
u</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"borde=
r-left: 1px solid rgb(204, 204, 204); margin: 0pt 0pt 0pt 0.8ex; padding-le=
ft: 1ex;">
Mike and Erick,<br>
<br>
In some of my work, I&#39;ve got code which is constantly creating and<br>
multiplying 4x4 matrices (Lorentz transforms). &nbsp;I usually write in a<b=
r>
functional style, so I do not generally overwrite old matrices with<br>
the multiplication results. &nbsp;I have discovered that, at these sizes,<b=
r>
it&#39;s about a factor of two to three faster to do this with OCaml<br>
arrays for the matrices rather than the Bigarrays you would need to<br>
use to interface with the BLAS in Lacaml or ocamlgsl. &nbsp;The reason is<b=
r>
that the memory block for a bigarray is malloc-ed, which is extremely<br>
slow compared to the OCaml native allocation. &nbsp;That overhead<br>
completely eliminates the advantage of going through C or Fortran<br>
libraries.<br>
<br>
The ideal solution would be to use OCaml native float arrays (fast<br>
allocation) with LAPACK (fast matrix multiply), but I&#39;m satisfied with<=
br>
the performance of my code, and too lazy to put out the effort to<br>
implement the C glue for this.<br>
<br>
Just thought you might appreciate another data point. &nbsp;I have no idea<=
br>
where the turnover is between 4x4 and 61x61 :).<br>
<br>
Will<br>
<br>
2009/2/20 Mike Lin &lt;<a href=3D"mailto:nilekim@gmail.com">nilekim@gmail.c=
om</a>&gt;:<br>
<div><div></div><div class=3D"Wj3C7c">&gt; Erick, we should compare notes s=
ometime. I have a lot of code for doing this<br>
&gt; kind of stuff (I am working on empirical codon models with 61x61 rate<=
br>
&gt; matrices). The right way to speed up matrix-vector operations is to us=
e BLAS<br>
&gt; via either Lacaml or ocamlgsl. But if, like me, you like to<br>
&gt; counterproductively fiddle around with stupid things, here&#39;s a lit=
tle ocaml<br>
&gt; library I wrote that links to a C function to do vector-vector dot usi=
ng<br>
&gt; SSE2 vectorization.<br>
&gt;<br>
&gt; <a href=3D"http://www.broad.mit.edu/%7Emlin/for_others/SuperFast_dot.z=
ip" target=3D"_blank">http://www.broad.mit.edu/~mlin/for_others/SuperFast_d=
ot.zip</a><br>
&gt;<br>
&gt; IIRC in my tests it gave about 50% speedup vs. the obvious C code and =
100%<br>
&gt; speedup vs. the obvious ocaml code. But, I am doing 61x61 and I&#39;m =
not sure<br>
&gt; if the speedup scales down to 20x20 or especially 4x4.<br>
&gt;<br>
&gt; Mike<br>
&gt;<br>
&gt; On Fri, Feb 20, 2009 at 2:53 PM, Erick Matsen &lt;<a href=3D"mailto:ma=
tsen@berkeley.edu">matsen@berkeley.edu</a>&gt; wrote:<br>
&gt;&gt;<br>
&gt;&gt; Wow, once again I am amazed by the vitality of this list. Thank yo=
u<br>
&gt;&gt; for your suggestions.<br>
&gt;&gt;<br>
&gt;&gt; Here is the context: we are interested in calculating the likeliho=
od<br>
&gt;&gt; of taxonomic placement of short &quot;metagenomics&quot; sequence =
fragments from<br>
&gt;&gt; unknown organisms in the ocean. We start by assuming a model of<br=
>
&gt;&gt; sequence evolution, which is a reversible Markov chain. The taxono=
my<br>
&gt;&gt; is represented as a tree, and the sequence information is a collec=
tion<br>
&gt;&gt; of likelihoods of sequence identities. As we move up the tree, the=
se<br>
&gt;&gt; sequences &quot;evolve&quot; by getting multiplied by the exponent=
iated<br>
&gt;&gt; instantaneous Markov matrix.<br>
&gt;&gt;<br>
&gt;&gt; The matrices are of the size of the sequence model: 4x4 when looki=
ng<br>
&gt;&gt; at nucleotides, and 20x20 when looking at proteins.<br>
&gt;&gt;<br>
&gt;&gt; The bottleneck is (I mis-spoke before) that we are multiplying man=
y<br>
&gt;&gt; length-4 or length-20 vectors by a collection of matrices which<br=
>
&gt;&gt; represent the time evolution of those sequences as follows.<br>
&gt;&gt;<br>
&gt;&gt; Outer loop:<br>
&gt;&gt; &nbsp;modify the amount of time each markov process runs<br>
&gt;&gt; &nbsp;exponentiate the rate matrices to get transition matrices<br=
>
&gt;&gt;<br>
&gt;&gt; &nbsp;Recur over the tree, starting at the leaves:<br>
&gt;&gt; &nbsp; &nbsp;at a node, multiply all of the daughter likelihood ve=
ctors together<br>
&gt;&gt; &nbsp; &nbsp;return the multiplication of that product by the tras=
ition matrix<br>
&gt;&gt; (bottleneck!)<br>
&gt;&gt;<br>
&gt;&gt; The trees are on the order of 50 leaves, and there are about 500<b=
r>
&gt;&gt; likelihood vectors done at once.<br>
&gt;&gt;<br>
&gt;&gt; All of this gets run on a big cluster of Xeons. It&#39;s not worth=
<br>
&gt;&gt; parallelizing because we are running many instances of this proces=
s<br>
&gt;&gt; already, which fills up the cluster nodes.<br>
&gt;&gt;<br>
&gt;&gt; So far I have been doing the simplest thing possible, which is jus=
t to<br>
&gt;&gt; multiply the matrices out like \sum_j a_ij v_j. Um, this is a bit<=
br>
&gt;&gt; embarassing.<br>
&gt;&gt;<br>
&gt;&gt; let mul_vec m v =3D<br>
&gt;&gt; &nbsp; &nbsp;if Array.length v &lt;&gt; n_cols m then<br>
&gt;&gt; &nbsp; &nbsp; &nbsp;failwith &quot;mul_vec: matrix size and vector=
 size don&#39;t match!&quot;;<br>
&gt;&gt; &nbsp; &nbsp;let result =3D Array.create (n_rows m) N.zero in<br>
&gt;&gt; &nbsp; &nbsp;for i=3D0 to (n_rows m)-1 do<br>
&gt;&gt; &nbsp; &nbsp; &nbsp;for j=3D0 to (n_cols m)-1 do<br>
&gt;&gt; &nbsp; &nbsp; &nbsp; &nbsp;result.(i) &lt;- N.add result.(i) (N.mu=
l (get m i j) v.(j))<br>
&gt;&gt; &nbsp; &nbsp; &nbsp;done;<br>
&gt;&gt; &nbsp; &nbsp;done;<br>
&gt;&gt; &nbsp; &nbsp;result<br>
&gt;&gt;<br>
&gt;&gt; I have implemented it in a functorial way for flexibility. N is th=
e<br>
&gt;&gt; number class. How much improvement might I hope for if I make a<br=
>
&gt;&gt; dedicated float vector multiplication function? I&#39;m sorry, I k=
now<br>
&gt;&gt; nothing about &quot;boxing.&quot; Where can I read about that?<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; Thank you again,<br>
&gt;&gt;<br>
&gt;&gt; Erick<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt;<br>
&gt;&gt; On Fri, Feb 20, 2009 at 10:46 AM, Xavier Leroy &lt;<a href=3D"mail=
to:Xavier.Leroy@inria.fr">Xavier.Leroy@inria.fr</a>&gt;<br>
&gt;&gt; wrote:<br>
&gt;&gt; &gt;&gt; I&#39;m working on speeding up some code, and I wanted to=
 check with<br>
&gt;&gt; &gt;&gt; someone before implementation.<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; As you can see below, the code primarily spends its time =
multiplying<br>
&gt;&gt; &gt;&gt; relatively small matrices. Precision is of course importa=
nt but not<br>
&gt;&gt; &gt;&gt; an incredibly crucial issue, as the most important thing =
is relative<br>
&gt;&gt; &gt;&gt; comparison between things which *should* be pretty differ=
ent.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; You need to post your matrix multiplication code so that the =
regulars<br>
&gt;&gt; &gt; on this list can tear it to pieces :-)<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; From the profile you gave, it looks like you parameterized yo=
ur matrix<br>
&gt;&gt; &gt; multiplication code over the + and * operations over matrix e=
lements.<br>
&gt;&gt; &gt; This is good for genericity but not so good for performance, =
as it<br>
&gt;&gt; &gt; will result in more boxing (heap allocation) of floating-poin=
t values.<br>
&gt;&gt; &gt; The first thing you should try is write a version of matrix<b=
r>
&gt;&gt; &gt; multiplication that is specialized for type &quot;float&quot;=
.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Then, there are several ways to write the textbook matrix<br>
&gt;&gt; &gt; multiplication algorithm, some of which perform less boxing t=
han<br>
&gt;&gt; &gt; others. &nbsp;Again, post your code and we&#39;ll let you kno=
w.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;&gt; Currently I&#39;m just using native (double-precision) oc=
aml floats and<br>
&gt;&gt; &gt;&gt; the native ocaml arrays for a first pass on the problem. =
&nbsp;Now I&#39;m<br>
&gt;&gt; &gt;&gt; thinking about moving to using float32 bigarrays, and I&#=
39;m hoping<br>
&gt;&gt; &gt;&gt; that the code will double in speed. I&#39;d like to know:=
 is that<br>
&gt;&gt; &gt;&gt; realistic? Any other suggestions?<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; It won&#39;t double in speed: arithmetic operations will take=
 exactly the<br>
&gt;&gt; &gt; same time in single or double precision. &nbsp;What single-pr=
ecision<br>
&gt;&gt; &gt; bigarrays buy you is halving the memory footprint of your mat=
rices.<br>
&gt;&gt; &gt; That could result in better cache behavior and therefore slig=
htly<br>
&gt;&gt; &gt; better speed, but it depends very much on the sizes and numbe=
r of your<br>
&gt;&gt; &gt; matrices.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; - Xavier Leroy<br>
&gt;&gt; &gt;<br>
&gt;&gt;<br>
&gt;&gt; _______________________________________________<br>
&gt;&gt; Caml-list mailing list. Subscription management:<br>
&gt;&gt; <a href=3D"http://yquem.inria.fr/cgi-bin/mailman/listinfo/caml-lis=
t" target=3D"_blank">http://yquem.inria.fr/cgi-bin/mailman/listinfo/caml-li=
st</a><br>
&gt;&gt; Archives: <a href=3D"http://caml.inria.fr" target=3D"_blank">http:=
//caml.inria.fr</a><br>
&gt;&gt; Beginner&#39;s list: <a href=3D"http://groups.yahoo.com/group/ocam=
l_beginners" target=3D"_blank">http://groups.yahoo.com/group/ocaml_beginner=
s</a><br>
&gt;&gt; Bug reports: <a href=3D"http://caml.inria.fr/bin/caml-bugs" target=
=3D"_blank">http://caml.inria.fr/bin/caml-bugs</a><br>
&gt;<br>
&gt;<br>
&gt; _______________________________________________<br>
&gt; Caml-list mailing list. Subscription management:<br>
&gt; <a href=3D"http://yquem.inria.fr/cgi-bin/mailman/listinfo/caml-list" t=
arget=3D"_blank">http://yquem.inria.fr/cgi-bin/mailman/listinfo/caml-list</=
a><br>
&gt; Archives: <a href=3D"http://caml.inria.fr" target=3D"_blank">http://ca=
ml.inria.fr</a><br>
&gt; Beginner&#39;s list: <a href=3D"http://groups.yahoo.com/group/ocaml_be=
ginners" target=3D"_blank">http://groups.yahoo.com/group/ocaml_beginners</a=
><br>
&gt; Bug reports: <a href=3D"http://caml.inria.fr/bin/caml-bugs" target=3D"=
_blank">http://caml.inria.fr/bin/caml-bugs</a><br>
&gt;<br>
&gt;<br>
</div></div></blockquote></div><br>

--0015175cde66c586f50463616295--

