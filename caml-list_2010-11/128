Return-Path: <examachine@gmail.com>
X-Original-To: caml-list@yquem.inria.fr
Delivered-To: caml-list@yquem.inria.fr
Received: from mail1-relais-roc.national.inria.fr (mail1-relais-roc.national.inria.fr [192.134.164.82])
	by yquem.inria.fr (Postfix) with ESMTP id 41DF9BC57
	for <caml-list@yquem.inria.fr>; Wed, 17 Nov 2010 00:04:56 +0100 (CET)
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: AoYDAOqc4kxKfVM0k2dsb2JhbACUMYYnAYc2TQgVAQEBAQkJCgkRAx+mVolighiFBS6IWQEBAwWFRgSKWA
X-IronPort-AV: E=Sophos;i="4.59,207,1288566000"; 
   d="scan'208";a="87658215"
Received: from mail-gw0-f52.google.com ([74.125.83.52])
  by mail1-smtp-roc.national.inria.fr with ESMTP; 17 Nov 2010 00:04:55 +0100
Received: by gwj22 with SMTP id 22so807482gwj.39
        for <caml-list@yquem.inria.fr>; Tue, 16 Nov 2010 15:04:54 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=gamma;
        h=domainkey-signature:mime-version:received:received:in-reply-to
         :references:date:message-id:subject:from:to:cc:content-type;
        bh=ONZrSet9DDg8+YN/scvi1esqPTRRh6Pc3rKTexWMS7k=;
        b=lz2pAyeiLNnFe1Fh0LXE6HD7+shaWb0G8Cd/ETR6xs2YWqmYEOfHohZ7A/W03bF3Ov
         v74blSDZ/0Qylv0dTzgfLxndK/atov4wauM+7WoFiqubg0WlBsHZMMLw2xFhIGvvZ0ui
         451TdGv9MwRxeimS5tM7diWuty2zZ+gJL9xIk=
DomainKey-Signature: a=rsa-sha1; c=nofws;
        d=gmail.com; s=gamma;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        b=miE4LvaPY8A3mlMaKa4wR+P0IeoAcL6XNRPyrbY/pLvOXFI7nkCaoMng501Ip44t/U
         cZ8tLX/XLKIImyz7cvpbkoib7o1NSyUon5CeLVrwQu5IZ8OXhpWpiQr9m8fl8vMkv7yR
         2G+2xsOvlNmX6oP9Cucdq5MNQVv8aKtQtUTwQ=
MIME-Version: 1.0
Received: by 10.90.75.18 with SMTP id x18mr10453299aga.5.1289948694201; Tue,
 16 Nov 2010 15:04:54 -0800 (PST)
Received: by 10.91.154.3 with HTTP; Tue, 16 Nov 2010 15:04:54 -0800 (PST)
In-Reply-To: <1289945605.16005.205.camel@thinkpad>
References: <20101115182737.42b8dcae@loki.yggdrasil.draxit.de>
	<4CE228CA.3030503@gmail.com>
	<1289927042.16005.176.camel@thinkpad>
	<AANLkTim2r0S7ZSmXFK87-u2PKRqvyuiBkH5rQfp7ju_C@mail.gmail.com>
	<1289945605.16005.205.camel@thinkpad>
Date: Wed, 17 Nov 2010 01:04:54 +0200
Message-ID: <AANLkTimgyL2xzUnuSzf-upSyjicffC-6-rgSK=EfxLTj@mail.gmail.com>
Subject: Re: [Caml-list] SMP multithreading
From: Eray Ozkural <examachine@gmail.com>
To: Gerd Stolpmann <info@gerd-stolpmann.de>
Cc: caml-list@yquem.inria.fr
Content-Type: multipart/alternative; boundary=0016361e7cf8b249a904953395a3
X-Spam: no; 0.00; eray:01 ozkural:01 gerd:01 stolpmann:01 eray:01 ozkural:01 gerd:01 stolpmann:01 compiler:01 runtime:01 runtime:01 compiler:01 subroutines:01 tackled:01 parallelism:01 

--0016361e7cf8b249a904953395a3
Content-Type: text/plain; charset=ISO-8859-1

On Wed, Nov 17, 2010 at 12:13 AM, Gerd Stolpmann <info@gerd-stolpmann.de>wrote:

> Am Dienstag, den 16.11.2010, 22:35 +0200 schrieb Eray Ozkural:
> >
> >
> > On Tue, Nov 16, 2010 at 7:04 PM, Gerd Stolpmann
> > <info@gerd-stolpmann.de> wrote:
> >         Am Montag, den 15.11.2010, 22:46 -0800 schrieb Edgar
> >         Friendly:
> >              * As somebody mentioned "implicit parallelization": Don't
> >         expect
> >                anything from this. Even if a good compiler finds ways
> >         to
> >                parallelize 20% of the code (which would be a lot), the
> >         runtime
> >                effect would be marginal. 80% of the code is run at
> >         normal speed
> >                (hopefully) and dominates the runtime behavior. The
> >         point is
> >                that such compiler-driven code improvements are only
> >         local
> >                optimizations. For getting good parallelization results
> >         you need
> >                to restructure the design of the program - well, maybe
> >                compiler2.0 can do this at some time, but this is not
> >         in sight.
> >
> > I think you are underestimating parallelizing compilers.
>
> I was more citing Amdahl's law, and did not want to criticize any effort
> in this area. It's more the usefulness for the majority of problems. How
> useful is the best parallelizing compiler if only a small part of the
> program _can_ actually benefit from it? Think about it. If you are not
> working in an area where many subroutines can be sped up, you consider
> this way of parallelizing as a waste of time. And this is still true for
> the majority of problems. Also, for many problems that can be tackled,
> the scalability is very limited.
>


What makes you think only 20% of the code can be parallelized? I've worked
in such a compiler project, and there were way too many opportunities for
parallelization in ordinary C code, let alone a functional language;
implicit parallelism would work wonders there. Of course you may think
whatever you were thinking, but I know that a high degree of parallelism can
be achieved through a functional language. I can't tell you much more
though. If you think that a very small portion of the code can be
parallelized you probably do not appreciate the kinds of static and dynamic
analysis those compilers perform. Of course if you are thinking of applying
it to some office or e-mail application, this might not be the case, but
automatic parallelization strategies would work best when you apply them to
a computationally intensive program.

The really limiting factor for current functional languages would be their
reliance on inherently sequential primitives like list processing, which may
in some cases limit the compiler to only pipeline parallelism (something
non-trivial to do by hand actually). Instead they would have to get their
basic forms from high-level parallel PLs (which might mean rewriting a lot
of things). The programs could look like something out of a category theory
textbook then. But I think even without modification you could get a lot of
speedup from ocaml code by applying state-of-the-art automatic
parallelization techniques. By the way, how much of the serial work is
parallelized that's what matters rather than the code length that is. Even
if the parallelism is not so obvious, the analysis can find out which
iterations are parallelizable, which variables have which kinds of
dependencies, memory dependencies, etc. etc. In the public, there is this
perception that automatic parallelization does not work, which is wrong,
while it is true that the popular compiler vendors do not understand much in
this regard, all I've seen (in popular products) are lame attempts to
generate vector instructions....

That is not to say, that implicit parallelization of current code can
replace parallel algorithm design (which is AI-complete). Rather, I think,
implicit parallelization is one of the things that will help parallel
computing people: by having them work at a more abstract level, exposing
more concurrency through functional forms, yet avoiding writing low-level
comms code. High-level explicit parallelism is also quite favorable, as
there may be many situations where, say, dynamic load-balancing approaches
are suitable. The approach of HPF might be relevant here, with the
programmer making annotations to guide the distribution of data structures,
and the rest inferred from code.

So whoever says that isn't possible, probably hasn't read up much in the
computer architecture community. Probably even expert PL researchers may be
misled here, as they have made the quoted remark or similar remarks about
multi-core/SMP  architectures. It was known for a very long time (20+ years)
that the clock speeds would hit a wall and then we'd have to expend more
area. This is true regardless of the underlying architecture/technology
actually. Mother nature is parallel. It is the sequence that is an
abstraction. And I wonder what is more natural than expressing a solution to
a problem in terms of functions and sets? The kind of non-leaky abstractions
required can be provided by a type-safe functional language and then the
compiler will have a lot more to work on than C code, although analysis can
reveal much about even C code. With the correct heuristics, it might decide
on good data and task distribution strategies, translating say a set
constructor notation to an efficient parallel algorithm, why not??

What I say applies to parallelization based on analysis, of course for
functional languages, other parallelization strategies are also possible, I
suppose low-level task parallelization and dynamic load-balancing strategies
(where functions or closures are distributed) are the most popular (the
reasoning being there can be many independent function evaluations
concurrently), I wonder if it has been attempted at all for ocaml? The
impurity of the language can be dealt with easily. And is there anyone who
might make suggestions to somebody who would like to work on automatic
parallelization of ocaml code? I actually think it might be fun to try some
of the simpler strategies and see how they yield on current hardware
platforms like multi-core and GPU. We ideally wouldn't like just good
automatic parallelization of, say, linear algebra code, but also cool stuff
like functional data structures. Recursive procedures can be mapped to
threads, it might match best hand-written parallelizations, actually, and
you'd get platform independence as a bonus if the compiler is well-designed
:)

Best,

-- 
Eray Ozkural, PhD candidate.  Comp. Sci. Dept., Bilkent University, Ankara
http://groups.yahoo.com/group/ai-philosophy
http://myspace.com/arizanesil http://myspace.com/malfunct

--0016361e7cf8b249a904953395a3
Content-Type: text/html; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable

<br><br><div class=3D"gmail_quote">On Wed, Nov 17, 2010 at 12:13 AM, Gerd S=
tolpmann <span dir=3D"ltr">&lt;<a href=3D"mailto:info@gerd-stolpmann.de">in=
fo@gerd-stolpmann.de</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_qu=
ote" style=3D"margin: 0pt 0pt 0pt 0.8ex; border-left: 1px solid rgb(204, 20=
4, 204); padding-left: 1ex;">
Am Dienstag, den 16.11.2010, 22:35 +0200 schrieb Eray Ozkural:<br>
<div class=3D"im">&gt;<br>
&gt;<br>
&gt; On Tue, Nov 16, 2010 at 7:04 PM, Gerd Stolpmann<br>
&gt; &lt;<a href=3D"mailto:info@gerd-stolpmann.de">info@gerd-stolpmann.de</=
a>&gt; wrote:<br>
&gt; =A0 =A0 =A0 =A0 Am Montag, den 15.11.2010, 22:46 -0800 schrieb Edgar<b=
r>
&gt; =A0 =A0 =A0 =A0 Friendly:<br>
&gt; =A0 =A0 =A0 =A0 =A0 =A0 =A0* As somebody mentioned &quot;implicit para=
llelization&quot;: Don&#39;t<br>
&gt; =A0 =A0 =A0 =A0 expect<br>
&gt; =A0 =A0 =A0 =A0 =A0 =A0 =A0 =A0anything from this. Even if a good comp=
iler finds ways<br>
&gt; =A0 =A0 =A0 =A0 to<br>
&gt; =A0 =A0 =A0 =A0 =A0 =A0 =A0 =A0parallelize 20% of the code (which woul=
d be a lot), the<br>
&gt; =A0 =A0 =A0 =A0 runtime<br>
&gt; =A0 =A0 =A0 =A0 =A0 =A0 =A0 =A0effect would be marginal. 80% of the co=
de is run at<br>
&gt; =A0 =A0 =A0 =A0 normal speed<br>
&gt; =A0 =A0 =A0 =A0 =A0 =A0 =A0 =A0(hopefully) and dominates the runtime b=
ehavior. The<br>
&gt; =A0 =A0 =A0 =A0 point is<br>
&gt; =A0 =A0 =A0 =A0 =A0 =A0 =A0 =A0that such compiler-driven code improvem=
ents are only<br>
&gt; =A0 =A0 =A0 =A0 local<br>
&gt; =A0 =A0 =A0 =A0 =A0 =A0 =A0 =A0optimizations. For getting good paralle=
lization results<br>
&gt; =A0 =A0 =A0 =A0 you need<br>
&gt; =A0 =A0 =A0 =A0 =A0 =A0 =A0 =A0to restructure the design of the progra=
m - well, maybe<br>
&gt; =A0 =A0 =A0 =A0 =A0 =A0 =A0 =A0compiler2.0 can do this at some time, b=
ut this is not<br>
&gt; =A0 =A0 =A0 =A0 in sight.<br>
&gt;<br>
&gt; I think you are underestimating parallelizing compilers.<br>
<br>
</div>I was more citing Amdahl&#39;s law, and did not want to criticize any=
 effort<br>
in this area. It&#39;s more the usefulness for the majority of problems. Ho=
w<br>
useful is the best parallelizing compiler if only a small part of the<br>
program _can_ actually benefit from it? Think about it. If you are not<br>
working in an area where many subroutines can be sped up, you consider<br>
this way of parallelizing as a waste of time. And this is still true for<br=
>
the majority of problems. Also, for many problems that can be tackled,<br>
the scalability is very limited.<br>=A0
<font color=3D"#888888"></font></blockquote><div><br>What makes you think o=
nly 20% of the code can be parallelized? I&#39;ve worked in such a compiler=
 project, and there were way too many opportunities for parallelization in =
ordinary C code, let alone a functional language; implicit parallelism woul=
d work wonders there. Of course you may think whatever you were thinking, b=
ut I know that a high degree of parallelism can be achieved through a funct=
ional language. I can&#39;t tell you much more though. If you think that a =
very small portion of the code can be parallelized you probably do not appr=
eciate the kinds of static and dynamic analysis those compilers perform. Of=
 course if you are thinking of applying it to some office or e-mail applica=
tion, this might not be the case, but automatic parallelization strategies =
would work best when you apply them to a computationally intensive program.=
<br>
<br>The really limiting factor for current functional languages would be th=
eir reliance on inherently sequential primitives like list processing, whic=
h may in some cases limit the compiler to only pipeline parallelism (someth=
ing non-trivial to do by hand actually). Instead they would have to get the=
ir basic forms from high-level parallel PLs (which might mean rewriting a l=
ot of things). The programs could look like something out of a category the=
ory textbook then. But I think even without modification you could get a lo=
t of speedup from ocaml code by applying state-of-the-art automatic paralle=
lization techniques. By the way, how much of the serial work is parallelize=
d that&#39;s what matters rather than the code length that is. Even if the =
parallelism is not so obvious, the analysis can find out which iterations a=
re parallelizable, which variables have which kinds of dependencies, memory=
 dependencies, etc. etc. In the public, there is this perception that autom=
atic parallelization does not work, which is wrong, while it is true that t=
he popular compiler vendors do not understand much in this regard, all I&#3=
9;ve seen (in popular products) are lame attempts to generate vector instru=
ctions....<br>
<br>That is not to say, that implicit parallelization of current code can r=
eplace parallel algorithm design (which is AI-complete). Rather, I think, i=
mplicit parallelization is one of the things that will help parallel comput=
ing people: by having them work at a more abstract level, exposing more con=
currency through functional forms, yet avoiding writing low-level comms cod=
e. High-level explicit parallelism is also quite favorable, as there may be=
 many situations where, say, dynamic load-balancing approaches are suitable=
. The approach of HPF might be relevant here, with the programmer making an=
notations to guide the distribution of data structures, and the rest inferr=
ed from code.<br>
<br>So whoever says that isn&#39;t possible, probably hasn&#39;t read up mu=
ch in the computer architecture community. Probably even expert PL research=
ers may be misled here, as they have made the quoted remark or similar rema=
rks about multi-core/SMP=A0 architectures. It was known for a very long tim=
e (20+ years) that the clock speeds would hit a wall and then we&#39;d have=
 to expend more area. This is true regardless of the underlying architectur=
e/technology actually. Mother nature is parallel. It is the sequence that i=
s an abstraction. And I wonder what is more natural than expressing a solut=
ion to a problem in terms of functions and sets? The kind of non-leaky abst=
ractions required can be provided by a type-safe functional language and th=
en the compiler will have a lot more to work on than C code, although analy=
sis can reveal much about even C code. With the correct heuristics, it migh=
t decide on good data and task=20
distribution strategies, translating say a set constructor notation to=20
an efficient parallel algorithm, why not??<br><br>What I say applies to par=
allelization based on analysis, of course for functional languages, other p=
arallelization strategies are also possible, I suppose low-level task paral=
lelization and dynamic load-balancing strategies (where functions or closur=
es are distributed) are the most popular (the reasoning being there can be =
many independent function evaluations concurrently), I wonder if it has bee=
n attempted at all for ocaml? The impurity of the language can be dealt wit=
h easily. And is there anyone who might make suggestions to somebody who wo=
uld like to work on automatic parallelization of ocaml code? I actually thi=
nk it might be fun to try some of the simpler strategies and see how they y=
ield on current hardware platforms like multi-core and GPU. We ideally woul=
dn&#39;t like just good automatic parallelization of, say, linear algebra c=
ode, but also cool stuff like functional data structures. Recursive procedu=
res can be mapped to threads, it might match best hand-written parallelizat=
ions, actually, and you&#39;d get platform independence as a bonus if the c=
ompiler is well-designed :) <br>
<br>Best,<br></div></div><br>-- <br>Eray Ozkural, PhD candidate.=A0 Comp. S=
ci. Dept., Bilkent University, Ankara<br><a href=3D"http://groups.yahoo.com=
/group/ai-philosophy">http://groups.yahoo.com/group/ai-philosophy</a><br><a=
 href=3D"http://myspace.com/arizanesil">http://myspace.com/arizanesil</a> <=
a href=3D"http://myspace.com/malfunct">http://myspace.com/malfunct</a><br>
<br>

--0016361e7cf8b249a904953395a3--

