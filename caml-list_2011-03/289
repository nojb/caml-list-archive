Received: from mail4-relais-sop.national.inria.fr (mail4-relais-sop.national.inria.fr [192.134.164.105])
	by walapai.inria.fr (8.13.6/8.13.6) with ESMTP id p2UIEAX3028198
	for <caml-list@sympa-roc.inria.fr>; Wed, 30 Mar 2011 20:14:10 +0200
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-Anti-Spam-Result: AvgCADJyk01KfVK2kGdsb2JhbACERqEECBQBAQEBCQkNBxQEIYh5mX+KGYJeAQWOKgEEgSeDTHeWFzo
X-IronPort-AV: E=Sophos;i="4.63,269,1299452400"; 
   d="scan'208";a="91557613"
Received: from mail-wy0-f182.google.com ([74.125.82.182])
  by mail4-smtp-sop.national.inria.fr with ESMTP/TLS/RC4-SHA; 30 Mar 2011 20:14:04 +0200
Received: by wyf23 with SMTP id 23so2409920wyf.27
        for <multiple recipients>; Wed, 30 Mar 2011 11:14:04 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=googlemail.com; s=gamma;
        h=domainkey-signature:from:to:cc:references:in-reply-to:subject:date
         :organization:message-id:mime-version:content-type
         :content-transfer-encoding:x-mailer:thread-index:content-language;
        bh=9vn3MBSqWt0Qx9WYBPnD106+T+poqjIfQ9LA/wC4NsY=;
        b=c6u7q8k80urKmh03/s8Y3KEsZBR8dX/QoRiIPp//hRJM76SPSsuG2xP6FiQAZos+qh
         COcgqXEjPF34b04sgPRzpSAKpvRusSZMPvYp8j0RY8VouuhpqhKuWfLRL+i/NWrU+Qb6
         kAGt+TmQmigLfNEUU0tpYwCNhz4zz8PkMDKpc=
DomainKey-Signature: a=rsa-sha1; c=nofws;
        d=googlemail.com; s=gamma;
        h=from:to:cc:references:in-reply-to:subject:date:organization
         :message-id:mime-version:content-type:content-transfer-encoding
         :x-mailer:thread-index:content-language;
        b=J2vBD51EZWCJtuBKXduA1Kog+YM/WVfBKVwHROx7UaMLrsFma/mjDnIOVXNCva7Stz
         2RK9yxKgFJAV3vY+jeuYxxUi/OvIUbFks08B4+gD2I54xPi6KGV5s4VSm9C/CFWcJOyX
         JkE9IhIyvh2M6KiQ/OqljME4ZCzuVAvJIIuUI=
Received: by 10.216.234.203 with SMTP id s53mr1674616weq.54.1301508789897;
        Wed, 30 Mar 2011 11:13:09 -0700 (PDT)
Received: from WinEight ([87.115.154.244])
        by mx.google.com with ESMTPS id t5sm159255wes.33.2011.03.30.11.13.07
        (version=SSLv3 cipher=OTHER);
        Wed, 30 Mar 2011 11:13:08 -0700 (PDT)
From: Jon Harrop <jonathandeanharrop@googlemail.com>
To: "'Dario Teixeira'" <darioteixeira@yahoo.com>,
        "'Fabrice Le Fessant'" <Fabrice.Le_fessant@inria.fr>
Cc: <caml-list@inria.fr>
References: <4D8C944A.9060601@inria.fr> <642177.15865.qm@web111503.mail.gq1.yahoo.com>
In-Reply-To: <642177.15865.qm@web111503.mail.gq1.yahoo.com>
Date: Wed, 30 Mar 2011 19:12:52 +0100
Organization: Flying Frog Consultancy
Message-ID: <041701cbef06$1769e810$463db830$@ffconsultancy.com>
MIME-Version: 1.0
Content-Type: text/plain;
	charset="utf-8"
X-Mailer: Microsoft Outlook 14.0
Thread-Index: AQFSTWdcX4+etDzfKygYuvw8sdGYipU5Xe4A
Content-Language: en-gb
Content-Transfer-Encoding: 8bit
X-MIME-Autoconverted: from quoted-printable to 8bit by walapai.inria.fr id p2UIEAX3028198
Subject: RE: [Caml-list] Efficient OCaml multicore -- roadmap?

Dario wrote:
> Fabrice wrote:
> > Of course, sharing structured mutable data between threads will not be
> > possible, but actually, it is a good thing if you want to write
> > correct programs ;-)
> 
> This is a good point that is often neglected in multicore discussions.

There be dragons.

There is a critical separation of concerns here. On one side you have parallel programming for multicores, the sole objective of which is to increase throughput on those machines. On the other side you have concurrent programming, which is primarily concerned with handling the complexity of non-determinism and often latency as well as throughput. These are two different concepts with different goals and different solutions.

Due to the memory hierarchy on a multicore, mutating shared data is basically essential if you want to get decent performance from a parallel program. If you don't do this, you end up with cache misses from all cores contending for shared memory and, therefore, no performance gain from more cores.

You can easily see this effect by considering the following embarrassingly-parallel serial program:

  Array.init 10000 (fun _ -> Array.init 10000 float)

This is trivial to parallelize in F#:

  Array.Parallel.init 10000 (fun _ -> Array.init 10000 float)

But I see just 2.4x speedup going from 1 to 8 cores by doing this because of the massive contention for shared memory. Specifically, the memory bandwidth on this 2x quadcore Xeon box is enough to feed 2.4 of these cores but not all 8 of them.

The Haskell guys got this wrong:

  http://flyingfrogblog.blogspot.com/2010/06/regular-shape-polymorphic-parallel.html

Their parallel Laplace code is a particularly good example because it uses the anti-pattern:

  for t=0 to m-1 do
    parallel for i=0 to n-1 do
      for j=0 to n-1 do
        ... xs.(i).(j) ...

This is bad because there is no correlation (aka "temporal locality") between the tasks spawned by the inner parallel "for" loop and the cores they run on. Consequently, each iteration of the outer serial "for" loop requires different data on different cores and the machine is swamped with cache misses so they see (surprise!) only 2.4x speedup on their 8-core Xeon when other solution get faster serial performance and 5-7x speedups on 8-core Xeons.

One solution that attains excellent performance on multicores optimizes the algorithm by reducing the asymptotic cache miss rate (aka "cache complexity") by decomposing the space-time (t, i, j) into trapezoids and relying upon an implementation that uses work-stealing queues where child tasks executed on the same core as their parent task with high probability. See the excellent paper "Cache oblivious stencil computations" by Matteo Frigo (of FFTW fame) for more details:

  http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.6578&rep=rep1&type=pdf

This style of parallel programming is now the defacto standard for multicores, with popular implementations provided by both the Intel TBB and Microsoft TPL (in .NET 4).

Mutating shared data is a bad idea in the context of concurrent programming where throughput is not important and handling non-determinism is usually the main headache. Here, you can sacrifice throughput by introducing abstractions that make it much easier to write correct programs. Abstractions like agents.

> What kind of language constructs would also be introduced in order to manage the concurrency between the multiple threads?

Just use message passing between agents. For a concrete implementation, look at the MailboxProcessor in the F# standard library.

Cheers,
Jon.



